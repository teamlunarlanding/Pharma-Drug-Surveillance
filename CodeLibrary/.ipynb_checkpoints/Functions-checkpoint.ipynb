{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1043dd-2ead-4793-a0db-0b84973ac47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords, words\n",
    "import string\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "punctuation = set(punctuation)\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import nbimporter\n",
    "from scipy.stats import skew\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from IPython.display import display, Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os\n",
    "\n",
    "# needed to import country codes generated in the processing file because it is an input in one of our functions\n",
    "\n",
    "\n",
    "# load NLTK words corpus for English\n",
    "\n",
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de4fe4c-8c68-4912-b32d-262e3e0ae1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    '''Function to process text fields.\n",
    "    Involves removing punctuation, tokenizing text, removing stopwords, lemmatizing tokens, folding to lowercase, removing any words \n",
    "    that are not in NLTK's word dictionary.'''\n",
    "    # Define punctuation set\n",
    "    punctuation = set(string.punctuation)\n",
    "    # define words\n",
    "    english_words = set(words.words())\n",
    "    # Add additional punctuation character\n",
    "    additional_punctuation = {'‘', '—', '“', '«'}\n",
    "    punctuation.update(additional_punctuation)\n",
    "\n",
    "    # Tokenize text using NLTK's word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation, and lemmatize tokens\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token.lower())\n",
    "        for token in tokens\n",
    "        if token.lower() not in sw and all(char not in punctuation for char in token)\n",
    "        # remove numerical tokens   \n",
    "        and not token.isdigit()\n",
    "        # remove tokens with just one character\n",
    "        and len(token) > 1 \n",
    "        and token not in {\n",
    "            'department', 'health', 'public', 'food', 'drug', 'administration',\n",
    "            'release', 'report', 'research', 'methodology', 'approach', 'certain',\n",
    "            'energy', 'commission', 'ultimately', 'finding', 'investigation', 'also',\n",
    "            'available', 'center', 'disease', 'control', 'us', 'federal', 'authority',\n",
    "            'rounding', 'register', 'determine', 'absence', 'presence', 'de', 'use',\n",
    "            'unless', 'work', 'article', 'editor', 'publication', 'since', 'upon',\n",
    "            'many', 'meet', 'every', 'one', 'two', 'three', 'four', 'five', 'six',\n",
    "            'seven', 'eight', 'ago', 'name', 'address'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # check if tokens are in NLTK's word list - do not include, if not\n",
    "    tokens_in_dictionary = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if token in english_words\n",
    "    ]\n",
    "    \n",
    "    return tokens_in_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55db2eff-9fe4-4e0a-9550-bc302086a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text with 2-token n-grams for language context\n",
    "def process_text_grams(text):\n",
    "    # Define punctuation set\n",
    "    punctuation = set(string.punctuation)\n",
    "    # Add additional punctuation character\n",
    "    punctuation.update({'‘'})\n",
    "\n",
    "    # Tokenize text using NLTK's word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and punctuation, and lemmatize tokens\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token.lower())\n",
    "        for token in tokens\n",
    "        if token.lower() not in sw and all(char not in punctuation for char in token)\n",
    "    ]\n",
    "    \n",
    "    # Generate bigrams\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    \n",
    "    # Combine tokens and bigrams into one list\n",
    "    combined_tokens = tokens + [' '.join(bigram) for bigram in bigrams]\n",
    "    \n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ab3fcf-f5dc-4934-9d43-e58ad2fbfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text processing function for the drug labels specifically\n",
    "def process_label_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Define punctuation set\n",
    "        punctuation = set(string.punctuation)\n",
    "        \n",
    "        # Replace punctuation with spaces\n",
    "        for p in punctuation:\n",
    "            text = text.replace(p, ' ')\n",
    "        \n",
    "        # Tokenize text using NLTK's word_tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and lemmatize tokens\n",
    "        sw = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(token.lower())\n",
    "            for token in tokens\n",
    "            if token.lower() not in sw\n",
    "        ]\n",
    "\n",
    "        # Remove duplicate tokens while maintaining order\n",
    "        seen = set()\n",
    "        unique_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                seen.add(token)\n",
    "                unique_tokens.append(token)\n",
    "\n",
    "        # Remove \"nan\" tokens if present\n",
    "        unique_tokens = [token for token in unique_tokens if token != 'nan']\n",
    "\n",
    "        # If the resulting list is empty, return pd.NA\n",
    "        if not unique_tokens:\n",
    "            return pd.NA\n",
    "        \n",
    "        return unique_tokens  # Return the list of tokens\n",
    "    else:\n",
    "        return text  # Return the original value if it's not a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517cbe15-a18d-4e7c-a8f6-479caf4c4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text processing function for the drug labels specifically\n",
    "def process_label_text_grams(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        # Define punctuation set\n",
    "        punctuation = set(string.punctuation)\n",
    "        \n",
    "        # Replace punctuation with spaces\n",
    "        for p in punctuation:\n",
    "            text = text.replace(p, ' ')\n",
    "        \n",
    "        # Tokenize text using NLTK's word_tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and lemmatize tokens\n",
    "        sw = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(token.lower())\n",
    "            for token in tokens\n",
    "            if token.lower() not in sw\n",
    "        ]\n",
    "\n",
    "        # Remove duplicate tokens while maintaining order\n",
    "        seen = set()\n",
    "        unique_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                seen.add(token)\n",
    "                unique_tokens.append(token)\n",
    "\n",
    "        # Remove \"nan\" tokens if present\n",
    "        unique_tokens = [token for token in unique_tokens if token != 'nan']\n",
    "\n",
    "        # If the resulting list is empty, return pd.NA\n",
    "        if not unique_tokens:\n",
    "            return pd.NA\n",
    "\n",
    "         # Generate bigrams\n",
    "        bigrams = list(ngrams(unique_tokens, 2))\n",
    "        \n",
    "        # Combine tokens and bigrams into one list\n",
    "        combined_tokens = unique_tokens + [' '.join(bigram) for bigram in bigrams]\n",
    "        \n",
    "        return combined_tokens  # Return the list of tokens\n",
    "   \n",
    "    else:\n",
    "        return text  # Return the original value if it's not a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e955a249-e302-4ecc-9550-5100d4894b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to make unique IDs for each table\n",
    "def add_sequential_index(df, index_col_name):\n",
    "\n",
    "    # Reset the index and rename the index column to input index_col_name\n",
    "    df = df.reset_index().rename(columns={\"index\": index_col_name})\n",
    "    \n",
    "    # Add 1 to index to start index from 1 instead of 0\n",
    "    df[index_col_name] = df[index_col_name] + 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31f40ec-ee9f-4218-85c9-9d2f2b073bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return count of NaN and proportion of NaN in each column for a dataframe\n",
    "def nan_info(df):\n",
    "    # Count # of NA values\n",
    "    nan_counts = df.isna().sum()\n",
    "    \n",
    "    # Calculate proportion of NA values\n",
    "    prop_null = (nan_counts / len(df)) * 100\n",
    "    \n",
    "    # Create a DataFrame to store the information\n",
    "    nan_info = pd.DataFrame({\n",
    "        'column_name': nan_counts.index,\n",
    "        'null_count': nan_counts.values,\n",
    "        'null_proportion': prop_null.values\n",
    "    })\n",
    "    \n",
    "    return nan_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d695e3-5511-472b-880d-e859e451f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values to null for now (simplifies type conversions & plotting)\n",
    "def na_to_null(df, column):\n",
    "    df[column] = df[column].replace('N/A', np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7e2a7c-45e0-453f-999f-9350eafb3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicates\n",
    "def remove_duplicates(tokens):\n",
    "    return list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94a9b69-41aa-4d4f-9b3d-aa349265eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicates and handle NaNs\n",
    "def remove_duplicates_nan(tokens):\n",
    "    if isinstance(tokens, list):\n",
    "        # Remove \"nan\" tokens if present\n",
    "        tokens = [token for token in tokens if token != 'nan']\n",
    "        \n",
    "        # Return pd.NA if the list is empty after removing \"nan\" tokens\n",
    "        if not tokens:\n",
    "            return pd.NA\n",
    "        return list(set(tokens))\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada36d7b-20fc-448b-b291-12017f556984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify the product type\n",
    "def classify_product_type(product_types):\n",
    "    if 'human otc' in product_types:\n",
    "        return 2\n",
    "    elif 'human prescription' in product_types:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f61d5e2-5f0f-448a-b923-382659de8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process medrap reaction terms\n",
    "# Remove spacing and replace with a period, lowercase all letters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba670829-cb7e-42a0-8d84-f81d2b2edc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map age units to years, based on code specified here: https://open.fda.gov/apis/drug/event/searchable-fields/\n",
    "def convert_to_years(age, unit):\n",
    "    if pd.isna(unit):  # Check if value is NaN/None\n",
    "        return np.nan\n",
    "    elif unit == 800:  # Decade\n",
    "        return age * 10\n",
    "    elif unit == 801:  # Year\n",
    "        return age\n",
    "    elif unit == 802:  # Month\n",
    "        return age / 12\n",
    "    elif unit == 803:  # Week\n",
    "        return age / 52\n",
    "    elif unit == 804:  # Day\n",
    "        return age / 365\n",
    "    elif unit == 805:  # Hour\n",
    "        return age / (365 * 24)\n",
    "    else:\n",
    "        return np.nan  # Return NaN for unknown units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d96b81-a661-4b2b-aa82-7afb6f6649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return boxplot of character length for object columns, as well as descriptive statistics of character length\n",
    "def plot_character_length(df, df_name):\n",
    "    #filter for object columns\n",
    "    documents_table_object_cols = df.select_dtypes(include=['object'])\n",
    "\n",
    "    # Calculate the number of characters in each column\n",
    "    character_counts = documents_table_object_cols.applymap(lambda x: len(str(x)))\n",
    "\n",
    "    # Generate boxplot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.boxplot(data=character_counts, color='hotpink', orient = 'h')\n",
    "    plt.title(f'Number of Characters in Each Object Column - {df_name}')\n",
    "    plt.ylabel('Column')\n",
    "    plt.xlabel('Number of Characters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Statistics Table\n",
    "    stats_table = character_counts.describe().transpose()\n",
    "    print(\"\\nDescriptive Statistics on Character Length:\")\n",
    "    print(stats_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d776e7b-f77b-43fb-838c-1b517d182ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for detecting upper outliers\n",
    "def examine_text_outliers(series):\n",
    "\n",
    "    # acquire the mean and standard deviation of string lengths\n",
    "    mean_length = series.str.len().mean()\n",
    "    std_length = series.str.len().std()\n",
    "\n",
    "    # calculate upper bound for outlier detection\n",
    "    upper_bound = mean_length + 2 * std_length \n",
    "\n",
    "    # identify rows with string lengths above the upper bound\n",
    "    upper_length_outliers = series[series.str.len() > upper_bound]\n",
    "\n",
    "    return upper_length_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871e91c8-7fb1-4786-ab74-a61f040a12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_company_text(text):\n",
    "    import re\n",
    "    import string\n",
    "    import pandas as pd\n",
    "    extra_abv = ['nldsp', 'usasp', 'company', 'bax', 'spo',\n",
    "                'ccaza', 'cinry', 'and', 'cansp', 'oxyc',\n",
    "            'scpr', 'gbrct', 'gbrsp', 'tjp', 'unk',\n",
    "            'frasp', 'brasp', 'sol', 'cbst','pmco',\n",
    "            'jpnct', 'frua', 'espct', 'pre',\n",
    "            'dsu', 'gmbh', 'dse', 'belsp', 'crisp',\n",
    "            'kdl', 'irlsp', 'mpi', 'avee', 'usani', \n",
    "            'sun', 'belct', 'itasp', 'hkgsp', 'argsp',\n",
    "                'aegr']\n",
    "\n",
    "    country_codes_df = pd.read_csv('../DataLibrary/DynamicReferenceCSVs/country_codes_clean.csv')\n",
    "    country_codes = country_codes_df['codes'].tolist()\n",
    "    if isinstance(text, str):\n",
    "        # Remove punctuation and replace with a space\n",
    "        text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        \n",
    "        # Remove all numerical characters and replce with a space\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "        # Tokenize words on whitespace\n",
    "        tokens = text.split()\n",
    "    \n",
    "        # Convert all characters to lowercase\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "        # Only retain tokens with 3 or more characters and remove 2-character country codes\n",
    "        tokens = [token for token in tokens if len(token) > 2 \n",
    "                  and token not in country_codes\n",
    "                 and token not in extra_abv]\n",
    "\n",
    "        # Update public health reporting entity labels\n",
    "        token_replacements = {\"phhy\": \"pubhosp\", \"pheh\": \"pubhosp\", \"phho\": \"pubhosp\", \"phfr\": \"pubhosp\"}\n",
    "        tokens = [token_replacements.get(token, token) for token in tokens]\n",
    "\n",
    "        # Update long manufacturer names\n",
    "    \n",
    "        # Replace entire token list if it contains \"ridgefield\"\n",
    "        if 'ridgefield' in tokens:\n",
    "            tokens = ['bi', 'pharmaceuticals']\n",
    "\n",
    "        # Replace entire token list if it contains both \"ge\" and \"healthcare\"\n",
    "        if 'ge' in tokens and 'healthcare' in tokens:\n",
    "            tokens = ['ge', 'healthcare']\n",
    "\n",
    "        # Alexion pharma inc.\n",
    "        if 'alexion' in tokens:\n",
    "            tokens = ['alexion']\n",
    "\n",
    "        # Assign pd.NA if the token list is empty\n",
    "        if not tokens:\n",
    "            return pd.NA\n",
    "    \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42398b65-c829-4967-88d6-67fff232c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean manufacturer text \n",
    "def clean_manufacturer_text(text_list):\n",
    "    import re\n",
    "    \n",
    "    if not text_list or not isinstance(text_list, list):\n",
    "        return pd.NA\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    for text in text_list:\n",
    "        if isinstance(text, str):\n",
    "            # Remove the words \"inc\" and \"llc\" ignoring case\n",
    "            text = re.sub(r'\\b(?:inc|llc|ltd|lp|corp|usa)\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Remove all punctuation except commas and replace with spaces\n",
    "            text = re.sub(f\"[{re.escape(string.punctuation.replace(',', ''))}]\", ' ', text)\n",
    "            \n",
    "            # Remove any instances of two commas in a row, and replace with just a single comma\n",
    "            text = re.sub(r',+', ',', text)\n",
    "\n",
    "            # Remove all spaces and replace with dashes\n",
    "            text = text.replace(' ', '-')\n",
    "\n",
    "            # Remove any instances of two or more dashes in a row, and replace with just a single dash\n",
    "            text = re.sub(r'-+', '-', text)\n",
    "\n",
    "            # Tokenize text by splitting on commas\n",
    "            tokens = text.split(',')\n",
    "\n",
    "            # Convert all tokens to lowercase\n",
    "            tokens = [token.lower().strip() for token in tokens]\n",
    "\n",
    "            # Append tokens to cleaned_tokens list\n",
    "            cleaned_tokens.extend(tokens)\n",
    "\n",
    "    # Remove any empty strings from the list and remaining dashes\n",
    "    cleaned_tokens = [token for token in cleaned_tokens if token and not re.fullmatch(r'-+', token)]\n",
    "\n",
    "    # Remove any country code or location strings\n",
    "\n",
    "    # Update long names to standardized abbreviations dictionary\n",
    "\n",
    "    # If the resulting token list is empty, assign pd.NA\n",
    "    if not cleaned_tokens:\n",
    "        return pd.NA\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31a5530e-8c21-4e25-9e4b-158fd37eb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, top_n=5, verbose=True):\n",
    "    from collections import Counter\n",
    "    # Flatten the list of tokens and filter out any float values\n",
    "    flat_tokens = [token for token in tokens if not isinstance(token, float)]\n",
    "    \n",
    "    # Calculate the total number of tokens\n",
    "    total_tokens = len(flat_tokens)\n",
    "    \n",
    "    # Calculate the number of unique tokens\n",
    "    num_unique_tokens = len(set(flat_tokens))\n",
    "    \n",
    "    # Calculate lexical diversity\n",
    "    lexical_diversity = num_unique_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    # Calculate the number of characters\n",
    "    num_characters = sum(len(token) for token in flat_tokens)\n",
    "    \n",
    "    # Calculate the average token length\n",
    "    avg_token_length = num_characters / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    # Calculate token length variance\n",
    "    token_lengths = [len(token) for token in flat_tokens]\n",
    "    token_length_variance = np.var(token_lengths)\n",
    "    \n",
    "    # Calculate token length standard deviation\n",
    "    token_length_std_dev = np.std(token_lengths)\n",
    "    \n",
    "    # Find the most common tokens\n",
    "    most_common_tokens = Counter(flat_tokens).most_common(top_n)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"There are {total_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        print(f\"The average token length is {avg_token_length:.3f} in the data.\")\n",
    "        print(f\"The variance of token lengths is {token_length_variance:.3f} in the data.\")\n",
    "        print(f\"The standard deviation of token lengths is {token_length_std_dev:.3f} in the data.\")\n",
    "        print(f\"The {top_n} most common tokens are {most_common_tokens} in the data.\")\n",
    "    \n",
    "    return [\n",
    "        total_tokens,\n",
    "        num_unique_tokens,\n",
    "        lexical_diversity,\n",
    "        num_characters,\n",
    "        avg_token_length,\n",
    "        token_length_variance,\n",
    "        token_length_std_dev,\n",
    "        most_common_tokens\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b726e4d-48bd-44aa-ba25-7ea4ebb4f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.0\n",
      "Standard Deviation: 3.1622776601683795\n",
      "Variance: 10.0\n",
      "Skewness: 0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_descriptives(column):\n",
    "    # Calculate mean\n",
    "    mean_value = column.mean()\n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_deviation = column.std()\n",
    "    \n",
    "    # Calculate variance\n",
    "    variance_value = column.var()\n",
    "    \n",
    "    # Calculate skewness\n",
    "    skewness_value = skew(column)\n",
    "    \n",
    "    return mean_value, std_deviation, variance_value, skewness_value\n",
    "\n",
    "# Example usage:\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'numbers': [2, 4, 6, 8, 10]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate descriptives for the 'numbers' column\n",
    "mean_val, std_dev, var, skewness = calculate_descriptives(df['numbers'])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean: {mean_val}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "print(f\"Variance: {var}\")\n",
    "print(f\"Skewness: {skewness}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffec08cb-09eb-4d26-bc4d-f35bdeadb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_descriptives(column):\n",
    "    \"\"\"Calculate descriptives for numerical column inputs\"\"\"\n",
    "    median_value = column.median()\n",
    "\n",
    "    mean_value = column.mean()\n",
    "        \n",
    "    std_deviation = column.std()\n",
    "    \n",
    "    variance_value = column.var()\n",
    "    \n",
    "    skewness_value = skew(column)\n",
    "    \n",
    "    q1 = column.quantile(0.25)\n",
    "    \n",
    "    q3 = column.quantile(0.75)\n",
    "    \n",
    "    # Prepare data for tabulation\n",
    "    headers = ['Statistic', 'Value']\n",
    "    data = [\n",
    "        ['Median (Md)', median_value],\n",
    "        ['Mean (x-bar)', mean_value],\n",
    "        ['Standard Deviation (s)', std_deviation],\n",
    "        ['Variance (s2)', variance_value],\n",
    "        ['Skewness', skewness_value],\n",
    "        ['First Quartile (Q1)', q1],\n",
    "        ['Third Quartile (Q3)', q3]\n",
    "    ]\n",
    "    \n",
    "    # Print the table\n",
    "    print(tabulate(data, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ba1aa3-8dad-4d86-a597-0c9e8f043192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drugs are currently in list format, clean to work with\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return ','.join(x)\n",
    "    elif isinstance(x, str):\n",
    "        return x  # Handle strings as needed\n",
    "    else:\n",
    "        return x  # Handle other types as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c825f25-8de9-49b3-839c-8ad34d88dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match text\n",
    "def contains_unique_value(text, unique_values_lower):\n",
    "    matched_texts = []\n",
    "    text_lower = text.lower()\n",
    "    for value in unique_values_lower:\n",
    "        if value in text_lower:\n",
    "            matched_texts.append(value)\n",
    "    return matched_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e43344d-4fd7-44c0-92cc-96902dbe619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert list to concatenated string\n",
    "def list_to_string(lst):\n",
    "    if isinstance(lst, list):\n",
    "        return ', '.join(lst)\n",
    "    else:\n",
    "        return lst  # Handle non-list values if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad7d3d4-82f7-42fa-adb4-a1c0278f5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_normalize_drugs(df_chunk, df2_synonyms, df2_names):\n",
    "    def drug_name_norms(row):\n",
    "        med_product_list = str(row['med_product']).split(' \\, ')\n",
    "        med_product_list = list(set(med_product_list))  # unique values\n",
    "        synonyms_list = list(set(' '.join(df2_synonyms).split(' \\| ')))  # unique values\n",
    "        match = process.extractOne(' '.join(med_product_list), synonyms_list)\n",
    "        if match and match[1] > 90:\n",
    "            original_row_index = df2_synonyms[df2_synonyms.str.contains(match[0], regex=False)].index\n",
    "            if not original_row_index.empty:\n",
    "                return df2_names.iloc[original_row_index[0]]\n",
    "        return None\n",
    "\n",
    "    df_chunk['drug_name_norm'] = df_chunk.apply(drug_name_norms, axis=1)\n",
    "    return df_chunk\n",
    "\n",
    "def parallel_worker(chunk, func, args):\n",
    "    return func(chunk, *args)\n",
    "\n",
    "def apply_parallel(df, func, args):\n",
    "    df_split = np.array_split(df, mp.cpu_count())\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        result = pd.concat(pool.starmap(parallel_worker, [(chunk, func, args) for chunk in df_split]))\n",
    "    return result\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "#    start_time = time.time()\n",
    "\n",
    "    # Apply the parallel processing\n",
    "#    result_table = apply_parallel(drugs_test, process_and_normalize_drugs, (adrecs_drugs['DRUG_SYNONYMS'], adrecs_drugs['DRUG_NAME']))\n",
    "\n",
    "    # Combine the result into a single column in the original table\n",
    "#    drugs_test['drug_name_norm'] = result_table['drug_name_norm']\n",
    "\n",
    "#    end_time = time.time()\n",
    "#    execution_time = end_time - start_time\n",
    "#    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fca8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(list1, list2):\n",
    "\n",
    "     # Convert inner lists to tuples to make them hashable\n",
    "    set1 = set(tuple(x) if isinstance(x, list) else x for x in list1)\n",
    "    set2 = set(tuple(x) if isinstance(x, list) else x for x in list2)\n",
    "    \n",
    "    # Find the intersection of the two sets\n",
    "    common_elements = set1.intersection(set2)\n",
    "    \n",
    "    # Convert the set back to a list\n",
    "    return list(common_elements)\n",
    "\n",
    "#list1 = manus_table['set_id'].tolist()\n",
    "#list2 = labels_test['set_id'].tolist()\n",
    "\n",
    "#matches = find_matches(list1, list2)\n",
    "#matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71efdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time it\n",
    "#start_time = time.time()\n",
    "\n",
    "# Create reverse mapping from set_id to ndc\n",
    "#reverse_mapping = {}\n",
    "#for ndc, set_ids in manu_ndc_set_dict.items():\n",
    "#    if isinstance(set_ids, list):\n",
    "#        for set_id in set_ids:\n",
    "#            reverse_mapping[set_id] = ndc\n",
    "\n",
    "# Function to fill NaN values in 'ndc' based on 'set_id'\n",
    "def fillna_ndc_with_setid(row, reverse_dict):\n",
    "    if pd.isna(row['ndc']) and row['set_id'] in reverse_dict:\n",
    "        return reverse_dict[row['set_id']]\n",
    "    return row['ndc']\n",
    "\n",
    "# Apply the function to fill NaN values in 'ndc'\n",
    "#labels_test['ndc'] = labels_test.apply(lambda row: fillna_ndc_with_setid(row, reverse_mapping), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c90dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output_list(output_list):\n",
    "    # List of specific values to be removed\n",
    "    values_to_remove = [\n",
    "        \"mg\", \"hr\", \"xxx\", \"aabc\", \"aacg\", \"aafi\", \"aand\", \"aaty\", \"aazg\", \n",
    "        \"acne\", \"acre\", \"acid\", \"acting\", \"action\", \"activated\", \"adult\", \n",
    "        \"advanced\", \"adventure\", \"aged\", \"agent\", \"aging\", \"american\", \n",
    "        \"anal\", \"anti\", \"area\", \"armpit\", \"asian\", \"assorted\", \"athlete\", \n",
    "        \"baby\", 'first', 'treatment', 'infection', 'normal','based',\n",
    "        'natural', 'heart', 'kidney', 'liver', 'control', 'effective',\n",
    "        'cation', 'scent', 'sugar', 'solution', 'control', 'delay',\n",
    "        'clean', 'white', 'cough', 'effect', 'normal', 'fresh', \n",
    "        'health', 'human', 'sweet', 'clear', 'effect', 'improve', \n",
    "        'powder', 'daily', 'release', 'quick', 'horse', 'muscle', \n",
    "        'sensitive', 'supplement', 'diabetic', 'diabetes', 'throat',\n",
    "        'blood', 'major', 'inc', 'support', 'level', 'whole', \n",
    "        'michigan', 'clotting', 'severe', 'protection', 'capsule', \n",
    "        'weight', 'height', 'professional', 'management', 'trigger',\n",
    "        'system', 'nerve', 'cattle', 'medicated', 'joint', 'complete', \n",
    "        'foundation', 'special', 'constipation', 'sting', 'influenza',\n",
    "        'virus', 'guard', 'strength', 'tendon', 'injection', 'thick',\n",
    "        'correct', 'deliver', 'pediatric', 'product', 'serious', \n",
    "        'pressure', 'relief', 'black', 'essential', 'ultimate', \n",
    "        'laboratory', 'safety', 'break', 'brain', 'official', \n",
    "        'refill', 'light', 'protect', 'medication', 'deliver', \n",
    "        'recovery', 'headache', 'immune', 'since', 'protect', \n",
    "        'model', 'allergy', 'smoking', 'leader', 'donor', 'direct',\n",
    "        'stomach', 'intestine', 'dietary', 'growth', 'order',\n",
    "        'supplemen', 'medical', 'protects', 'treat', 'clinical', 'prevent', 'common', 'table', 'symptom', 'treated', 'therapy', 'inspec',\n",
    "    'multi', 'active', 'child', 'ether', 'menta', 'ml', 'viola', 'small', 'equate', 'function', 'plant', 'tissue', 'cover',\n",
    "    'fever', 'perio', 'total', 'protein', 'ester', 'young', 'infected', 'factor', 'tablet', 'california', 'enhance', 'labeled',\n",
    "    'discus', 'water', 'surgical', 'house', 'inflammation', 'higher', 'salmon', 'breath', 'microbial', 'sanitary', 'broad',\n",
    "    'breast', 'infant', 'inhibitor', 'family', 'mouth', 'chest', 'salmonella', 'bacterial', 'lymph', 'antibiotic', 'complex',\n",
    "    'arthritis', 'right', 'formula', 'antimicrobial', 'fluid', 'education', 'elder', 'surface', 'hormone', 'pulmo', 'gluco',\n",
    "    'cheese', 'removal', 'urinary', 'super', 'exposed', 'routine', 'cholesterol', 'equivalent', 'double', 'marrow', 'compound',\n",
    "    'purpose', 'forming', 'listeria', 'ribes', 'physical', 'original', 'sterile', 'series', 'dressing', 'shell', 'unique',\n",
    "    'gland', 'breathing', 'vessel', 'value', 'extended', 'extra', 'liquid', 'monocytogenes', 'borate', 'sleep', 'injectable',\n",
    "    'toxin', 'finish', 'receptor', 'chloride', 'filled', 'external', 'regio', 'moving', 'female', 'arteria', 'energy', 'motion',\n",
    "    'sodium', 'balance', 'artery', 'sport', 'frozen', 'fighting', 'reproductive', 'ultra', 'rheum', 'candida', 'fruit', 'rheuma',\n",
    "    'indian', 'comfort', 'flush', 'restore', 'bladder', 'cutaneous', 'xtreme', 'travel', 'little', 'witch', 'minor', 'hepatitis',\n",
    "    'renes', 'collection', 'alcohol', 'aries', 'colon', 'prostate', 'paste', 'resistance', 'vitamin', 'continuous', 'relieve',\n",
    "    'hydrochloride', 'analgesic', 'european', 'fungal', 'linum', 'botulinum', 'fluor', 'coccus', 'english', 'silver', 'frida',\n",
    "    'pancreas', 'extreme', 'night', 'rheumatoid', 'nasal', 'calcium', 'dental', 'photo', 'delayed', 'genital', 'green',\n",
    "    'clostridium', 'elaps', 'damaged', 'johnson', 'solid', 'tuber', 'stick', 'element', 'inhalation', 'cream', 'oxygen',\n",
    "    'poison', 'chill', 'derived', 'xygen', 'abies', 'woman', 'defense', 'antibacterial', 'hydration', 'spf', 'sinus', 'repair',\n",
    "    'vaginal', 'orally', 'fatty', 'sweat', 'spring', 'sulfate', 'juice', 'transparent', 'stool', 'sulfat', 'rectal', 'touch',\n",
    "    'stress', 'german', 'vegetable', 'topica', 'ethyl', 'companion', 'smooth', 'topical', 'serum', 'maximum', 'blast', 'spray',\n",
    "    'digestive', 'recombinant', 'carbo', 'coagulation', 'northern', 'beta', 'artificial', 'tarte', 'lesion', 'staphylococcus',\n",
    "    'aureus', 'crushed', 'thyroid', 'grain', 'preparation', 'potassium', 'mexican', 'releasing', 'paris', 'organic', 'portable',\n",
    "    'bayer', 'mineral', 'lymphocyte', 'china', 'dissolve', 'ntric', 'oregon', 'chamber', 'acetate', 'waste', 'wound', 'methyl',\n",
    "    'extract', 'apple', 'ophth', 'protective', 'chicken', 'enhancement', 'yellow', 'oyster', 'antigen', 'herbal', 'herba',\n",
    "    'santa', 'refrigerated', 'synthetic', 'compressed', 'forest', 'dermal', 'silicon', 'nitrate', 'spleen', 'cleaning',\n",
    "    'superior', 'mixture', 'stimulation', 'interferon', 'flavor', 'pepper', 'simple', 'piece', 'fiber', 'pellet', 'strengthening',\n",
    "    'circulatory', 'ocean', 'esophagus', 'antifungal', 'vulva', 'butter', 'rescue', 'standardized', 'violet', 'semen', 'giant',\n",
    "    'morning', 'streptococcus', 'enlarged', 'stimulating', 'healing', 'shield', 'phosphate', 'omega', 'platinum', 'powered',\n",
    "    'coffee', 'strip', 'alpha', 'collagen', 'fresco', 'prime', 'mountain', 'berry', 'peanut', 'ketone', 'burning', 'pectin',\n",
    "    'victoria', 'suspension', 'donna', 'neuropathy', 'thirty', 'medulla', 'lactic', 'rectum', 'simply', 'pacific', 'intensive',\n",
    "    'spore', 'tinct', 'diarrheal', 'gastric', 'membrane', 'primer', 'mucosa', 'purified', 'remedy', 'yeast', 'campylobacter',\n",
    "    'sticker', 'kentucky', 'uterus', 'pseudo', 'summer', 'clarifying', 'candy', 'patch', 'longa', 'firming', 'hydrate',\n",
    "    'osteoarthritis', 'tooth', 'stimulant', 'everyday', 'lymphatic', 'steam', 'defining', 'choline', 'peptide', 'reliever',\n",
    "    'magnesium', 'proof', 'clearing', 'sanitizer', 'montana', 'holly', 'concentrate', 'esium', 'spectrum', 'creme', 'papaya',\n",
    "    'flavored', 'alfalfa', 'escherichia', 'carbohydrate', 'lingual', 'antiseptic', 'adrenal', 'iodine', 'balanced', 'ophthalmic',\n",
    "    'herpes', 'tartrate', 'immunomodulator', 'ginger', 'pneumoniae', 'relieving', 'brown', 'tanning', 'plantar', 'carbon',\n",
    "    'ozone', 'clover', 'dermatitis', 'clove', 'brucella', 'soluble', 'solub', 'regener', 'beauty', 'slimming', 'bruise', 'coated',\n",
    "    'oxide', 'hydrogel', 'tropical', 'caffeine', 'applicator', 'honey', 'pollen', 'aeruginosa', 'pseudomonas', 'patent',\n",
    "    'immature', 'bovine', 'protease', 'instant', 'roasted', 'aerosol', 'soybean', 'twist', 'orange', 'gonadotropin', 'depleted',\n",
    "    'scalp', 'stuffy', 'papilloma', 'succinate', 'barrier', 'cultured', 'cysteine', 'mycobacterium', 'mucus', 'sanitizing',\n",
    "    'aspart', 'coagulant', 'polymer', 'cerebral', 'glucagon', 'radish', 'ovine', 'bella', 'cardiaca', 'balancing', 'cloth',\n",
    "    'coconut', 'sheep', 'shade', 'homeopathic', 'bacillus', 'ammonia', 'fumarate', 'posterior', 'gluten', 'acetyl', 'umbilical',\n",
    "    'packet', 'enterococcus', 'facial', 'citrate', 'sunscreen', 'prenatal', 'wheat', 'hepar', 'liner', 'prostatic', 'preservative',\n",
    "    'antacid', 'daytime', 'marine', 'staining', 'autologous', 'sponge', 'hepatica', 'dimethyl', 'difficile', 'lupus', 'salix',\n",
    "    'electrolyte', 'culta', 'guinea', 'grape', 'typhi', 'salve', 'yersinia', 'sterilized', 'drainage', 'wellness', 'phenol',\n",
    "    'chocolate', 'rabbit', 'serotonin', 'hemorrhoid', 'eczema', 'nitri', 'magic', 'sirolimus', 'bitartrate', 'teeth', 'pineapple',\n",
    "    'shingle', 'aspartate', 'retinal', 'medicinal', 'psoriasis', 'square', 'menstrual', 'congestion', 'medium', 'ovaria',\n",
    "    'cartilage', 'almond', 'pertussis', 'lotion', 'allogeneic', 'interleukin', 'convenience', 'syrup', 'cypress', 'peach', 'vivus',\n",
    "    'amara', 'lipase', 'narcotic', 'triple', 'sunshine', 'gluconate', 'plane', 'timothy', 'drowsy', 'parvovirus', 'cortisol',\n",
    "    'combo', 'sputum', 'rinse', 'liquida', 'repository', 'monkey', 'lemon', 'glycol', 'hemorrhoidal', 'angiotensin', 'porcine',\n",
    "    'povidon', 'povidone', 'banana', 'faecalis', 'haemolyticus', 'fluoride', 'sulfur', 'musca', 'bubble', 'nickel', 'cobalt',\n",
    "    'mexicana', 'chromium', 'canti', 'olismo', 'mesylate', 'probiotic', 'hydrogen', 'hydrogenated', 'detergent', 'technetium',\n",
    "    'tilmanocept', 'shigella', 'crystal', 'mango', 'sacred', 'varicella', 'zoster', 'shampoo', 'outdoor', 'breakout', 'anticoagulant',\n",
    "    'sickness', 'bordetella', 'caring', 'brazil', 'barley', 'tomato', 'generator', 'deodorant', 'vitis', 'americana', 'dalteparin',\n",
    "    'fifty', 'urethra', 'cytomegalovirus', 'eardrop', 'glandula', 'caribbean', 'dioxide', 'glandular', 'tonsil', 'estradiol',\n",
    "    'genuine', 'anesthetic', 'fungus', 'chorionic', 'rubber', 'shaping', 'envelope', 'bandage', 'creamy', 'inactivated', 'emulsion',\n",
    "    'haemophilus', 'enhancer', 'broom', 'pyogenes', 'benefiting', 'chewable', 'gallbladder', 'cashew', 'klebsiella', 'clidinium',\n",
    "    'influenzae', 'hispana', 'handwashing', 'collagenase', 'handwash', 'leafy', 'erection', 'solucion', 'disinfecting', 'basil',\n",
    "    'hazel', 'olive', 'titanium', 'detox', 'renew', 'placenta', 'cleanse', 'bethlehem', 'globulin', 'secretion', 'hurricane',\n",
    "    'phoenix', 'animale', 'hydrolyzed', 'jelly', 'nighttime', 'tincture', 'burst', 'traveler', 'transdermal', 'skeleton', 'diethyl',\n",
    "    'stigma', 'smoothie', 'disinfectant', 'glycero', 'glycerol', 'lubricant', 'helicobacter', 'shake', 'saccharate', 'feminine',\n",
    "    'redwood', 'butyrate', 'phenyl', 'exoskeleton', 'blueberry', 'eyewash', 'granule', 'finasteride', 'moisturizer', 'camphor',\n",
    "    'nitrogen', 'resin', 'disodium', 'alginate', 'bilberry', 'diminishing', 'gallium', 'meniscus', 'chestnut', 'disintegrating',\n",
    "    'minus', 'wrinkle', 'mover', 'remover', 'toner', 'shark', 'premium', 'fibrocartilage', 'allium', 'tauri', 'taurine', 'bromide',\n",
    "    'colloid', 'alliu', 'phosphorous', 'venom', 'swimmer', 'furoate', 'lemonade', 'bubblegum', 'raspberry', 'fibroblast',\n",
    "    'keratinocytes', 'tazobactam', 'plumb', 'alder', 'concentrated', 'coronavirus', 'mycoplasma', 'stranded', 'nhn', 'indole',\n",
    "    'canis', 'cobra', 'icatibant', 'brava', 'cereus', 'propyl', 'propylene', 'hyaluronic', 'palladium', 'anorectal', 'epstein',\n",
    "    'cholera', 'polidocanol', 'tacrolimus', 'wiesbaden', 'varicose', 'anticavity', 'xanthine', 'blanco', 'mouthwash', 'urethral',\n",
    "    'vernal', 'belly', 'orchard', 'grass', 'burnetii', 'coxiella', 'cacao', 'comfrey', 'chickweed', 'simplex', 'corros',\n",
    "    'linezolid', 'maple', 'whitening', 'aspen', 'epidermis', 'follicle', 'mucor', 'eliglustat', 'calcitonin', 'phosphorus',\n",
    "    'chlamydia', 'bisulfate', 'tetanus', 'booster', 'tropicalis', 'krusei', 'parapsilosis', 'albicans', 'glabra', 'mirabegron',\n",
    "    'pegaspargase', 'chrysanthemi', 'erwinia', 'asparaginase', 'asparagine', 'croton', 'grapefruit', 'adrenalin', 'boosting',\n",
    "    'cassia', 'angustifolia', 'gondii', 'toxoplasma', 'fermented', 'sulph', 'sulphur', 'lantern', 'pomegranate', 'antioxidant',\n",
    "    'alanine', 'melatonin', 'maleate', 'polyvinyl', 'glargine', 'glutamine', 'helium', 'derivative', 'reducer', 'tcm', 'cooling',\n",
    "    'cedar', 'chlorine', 'bleach', 'miracle', 'citrus', 'enteriditis', 'asparagus', 'cyanide', 'nitrite', 'paint', 'ovary',\n",
    "    'ribose', 'rickettsia', 'benzyl', 'spinosad', 'spinosa', 'folate', 'folic', 'vulgaris', 'embryo', 'immunoglobulin', 'pituitary',\n",
    "    'propionate', 'saffron', 'garlic', 'mushroom', 'cucumber', 'chlorothiazide', 'malvin', 'quinine', 'passion', 'chelate',\n",
    "    'detoxifying', 'massage', 'classic', 'vardenafil', 'tattoo', 'carboplatin', 'apricot', 'cinnamon', 'polysaccharide', 'foaming',\n",
    "    'protectant', 'calfactant', 'morgan', 'palmitate', 'fingolimod', 'tigan', 'ticagrelor', 'copper', 'octyl', 'sipuleucel',\n",
    "    'borage', 'nitric', 'geranium', 'strawberry', 'melon', 'epithelium', 'bimatoprost', 'premier', 'curve', 'murine', 'glucosamine',\n",
    "    'teething', 'belladonna', 'cortex', 'atropine', 'toothpaste', 'sturgeon', 'resurfacing', 'cellulose', 'gentamicin', 'gamma',\n",
    "    'oleum', 'petroleum', 'earwax', 'picosulfate', 'bisacodyl', 'citric', 'malus', 'midodrine', 'duodenum', 'albumin', 'album',\n",
    "    'meglumine', 'rosewood', 'hawthorn', 'perfect', 'overnight', 'cryptosporidium', 'giardia', 'speciosa', 'refresh', 'ointment',\n",
    "    'mulberry', 'coleus', 'bitter', 'lotus', 'marum', 'foenum', 'trigonella', 'hexafluoride', 'microspheres', 'radium', 'ibandronate',\n",
    "    'scripta', 'walnut', 'dandelion', 'artemisiifolia', 'ambrosia', 'roflumilast', 'butal'     \n",
    "    ]\n",
    "    \n",
    "    # Remove numbers and specific values from each string in the list\n",
    "    cleaned_list = []\n",
    "    for item in output_list:\n",
    "        # Remove all numbers\n",
    "        item_no_numbers = re.sub(r'\\d+', '', item)\n",
    "        # Remove specific values\n",
    "        if item_no_numbers not in values_to_remove:\n",
    "            cleaned_list.append(item_no_numbers)\n",
    "    \n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73a65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_drugs(text, unique_drug_names):\n",
    "    matched_drugs = []\n",
    "    for drug in unique_drug_names:\n",
    "        if drug.lower() in text.lower():\n",
    "            matched_drugs.append(drug)\n",
    "    return matched_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1d86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for matches between text and unique_reactions\n",
    "def check_reactions(text, unique_reactions):\n",
    "    text_lower = text.lower()\n",
    "    matches = [1 if reaction.lower() in text_lower else 0 for reaction in unique_reactions]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1a2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import json\n",
    "#import re\n",
    "#import logging\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "#logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "#def clean_json_string(json_str):\n",
    "#    if json_str is None:\n",
    "#        return None\n",
    "#    # Replace single quotes with double quotes\n",
    "#    json_str = json_str.replace(\"'\", '\"')\n",
    "#    \n",
    "#    # Remove trailing commas before closing brackets\n",
    "#    json_str = re.sub(r',(\\s*[\\]}])', r'\\1', json_str)\n",
    "#    \n",
    "#    # Remove extra commas within lists\n",
    "#    json_str = re.sub(r',(\\s*[\\]])', r'\\1', json_str)\n",
    "#    \n",
    "#    # Ensure that escaped quotes are correctly formatted\n",
    "#    json_str = re.sub(r'\\\\\\\"', '\"', json_str)\n",
    "#    \n",
    "#    return json_str\n",
    "\n",
    "#def safely_parse_json(json_str):\n",
    "#    if json_str is None:\n",
    "#        return None\n",
    "#    try:\n",
    "#        json_str = clean_json_string(json_str)\n",
    "#        return json.loads(json_str)\n",
    "#    except json.JSONDecodeError as e:\n",
    "#        logging.error(f\"Error parsing JSON: {e}\")\n",
    "#        snippet = json_str[:1000]\n",
    "#        logging.error(f\"Problematic JSON snippet: {snippet}...\")\n",
    "#        return None\n",
    "\n",
    "#def flatten_dict(d, parent_key='', sep='_'):\n",
    "#    items = []\n",
    "#    for k, v in d.items():\n",
    "#        new_key = f\"{parent_key}{k}\" if parent_key == '' else f\"{parent_key}{sep}{k}\"\n",
    "#        if isinstance(v, dict):\n",
    "#            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "#        elif isinstance(v, list):\n",
    "#            for i, item in enumerate(v):\n",
    "#                if isinstance(item, dict):\n",
    "#                    items.extend(flatten_dict(item, f\"{new_key}{sep}{i}\", sep=sep).items())\n",
    "#                else:\n",
    "#                    items.append((f\"{new_key}{sep}{i}\", item))\n",
    "#        else:\n",
    "#            items.append((new_key, v))\n",
    "#    return dict(items)\n",
    "\n",
    "#def extract_and_flatten_drug(row):\n",
    "#    event_id = row['event_id']\n",
    "#    drug_entries = row['drug']\n",
    "#    \n",
    "#    if not isinstance(drug_entries, list):\n",
    "#        return []\n",
    "    \n",
    "#    expanded_entries = []\n",
    "#    for drug_entry in drug_entries:\n",
    "#        if isinstance(drug_entry, dict):\n",
    "#            flattened_entry = flatten_dict(drug_entry)\n",
    "#            flattened_entry['event_id'] = event_id\n",
    "#            expanded_entries.append(flattened_entry)\n",
    "    \n",
    "#    return expanded_entries\n",
    "\n",
    "# Apply the JSON cleaning and parsing to the 'drug' column\n",
    "#events_table_subset['drug'] = events_table_subset['drug'].apply(\n",
    "#    lambda x: safely_parse_json(x) if isinstance(x, str) else x\n",
    "#)\n",
    "\n",
    "# Extract and flatten all columns from the 'drug' dictionary, keeping 'event_id'\n",
    "#expanded_data = []\n",
    "#for _, row in events_table_subset.iterrows():\n",
    "#    entries = extract_and_flatten_drug(row)\n",
    "#    expanded_data.extend(entries)\n",
    "\n",
    "# Convert the list of expanded entries to a DataFrame\n",
    "#patient_drugs_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "# Reorder columns to ensure 'event_id' is the first column\n",
    "#if not patient_drugs_df.empty and 'event_id' in patient_drugs_df.columns:\n",
    "    # Reorder columns with 'event_id' as the first column\n",
    "#    columns = ['event_id'] + [col for col in patient_drugs_df.columns if col != 'event_id']\n",
    "#    patient_drugs_df = patient_drugs_df[columns]\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(patient_drugs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c464af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import json\n",
    "#import re\n",
    "#import logging\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "#logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "#def clean_json_string(json_str):\n",
    "#    if json_str is None:\n",
    "#        return None\n",
    "#    # Replace single quotes with double quotes\n",
    "#    json_str = json_str.replace(\"'\", '\"')\n",
    "    \n",
    "    # Remove trailing commas before closing brackets\n",
    "#    json_str = re.sub(r',(\\s*[\\]}])', r'\\1', json_str)\n",
    "    \n",
    "    # Remove extra commas within lists\n",
    "#    json_str = re.sub(r',(\\s*[\\]])', r'\\1', json_str)\n",
    "    \n",
    "    # Ensure that escaped quotes are correctly formatted\n",
    "#    json_str = re.sub(r'\\\\\\\"', '\"', json_str)\n",
    "    \n",
    "#    return json_str\n",
    "\n",
    "#def safely_parse_json(json_str):\n",
    "#    if json_str is None:\n",
    "#        return None\n",
    "#    try:\n",
    "#        json_str = clean_json_string(json_str)\n",
    "#        return json.loads(json_str)\n",
    "#    except json.JSONDecodeError as e:\n",
    "#        logging.error(f\"Error parsing JSON: {e}\")\n",
    "#        snippet = json_str[:1000]\n",
    "#        logging.error(f\"Problematic JSON snippet: {snippet}...\")\n",
    "#        return None\n",
    "\n",
    "#def flatten_dict(d, parent_key='', sep='_'):\n",
    "#    items = []\n",
    "#    for k, v in d.items():\n",
    "#        new_key = f\"{parent_key}{k}\" if parent_key == '' else f\"{parent_key}{sep}{k}\"\n",
    "#        if isinstance(v, dict):\n",
    "#            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "#        elif isinstance(v, list):\n",
    "#            for i, item in enumerate(v):\n",
    "#                if isinstance(item, dict):\n",
    "#                    items.extend(flatten_dict(item, f\"{new_key}{sep}{i}\", sep=sep).items())\n",
    "#                else:\n",
    "#                    items.append((f\"{new_key}{sep}{i}\", item))\n",
    "#        else:\n",
    "#            items.append((new_key, v))\n",
    "#    return dict(items)\n",
    "\n",
    "#def extract_and_flatten_reaction(row):\n",
    "#    event_id = row['event_id']\n",
    "#    reaction_entries = row['reaction']\n",
    "    \n",
    "#    if not isinstance(reaction_entries, list):\n",
    "#        return []\n",
    "    \n",
    "#    expanded_entries = []\n",
    "#    for reaction_entry in reaction_entries:\n",
    "#        if isinstance(reaction_entry, dict):\n",
    "#            flattened_entry = flatten_dict(reaction_entry)\n",
    "#            flattened_entry['event_id'] = event_id\n",
    "#            expanded_entries.append(flattened_entry)\n",
    "    \n",
    "#    return expanded_entries\n",
    "\n",
    "# Apply the JSON cleaning and parsing to the 'reaction' column\n",
    "#events_table_subset['reaction'] = events_table_subset['reaction'].apply(\n",
    "#    lambda x: safely_parse_json(x) if isinstance(x, str) else x\n",
    "#)\n",
    "\n",
    "# Extract and flatten all columns from the 'reaction' dictionary, keeping 'event_id'\n",
    "#expanded_data = []\n",
    "#for _, row in events_table_subset.iterrows():\n",
    "#    entries = extract_and_flatten_reaction(row)\n",
    "#    expanded_data.extend(entries)\n",
    "\n",
    "# Convert the list of expanded entries to a DataFrame\n",
    "#patient_reactions_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "# Reorder columns to ensure 'event_id' is the first column\n",
    "#if not patient_reactions_df.empty and 'event_id' in patient_reactions_df.columns:\n",
    "    # Reorder columns with 'event_id' as the first column\n",
    "#    columns = ['event_id'] + [col for col in patient_reactions_df.columns if col != 'event_id']\n",
    "#    patient_reactions_df = patient_reactions_df[columns]\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(patient_reactions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd32c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, labels, model_name):\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None, labels=labels)\n",
    "    recall = recall_score(y_test, y_pred, average=None, labels=labels)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, labels=labels)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    \n",
    "    # Calculate specificity for each class\n",
    "    specificity = np.array([\n",
    "        (np.sum(cm) - np.sum(cm[:, i]) - np.sum(cm[i, :]) + cm[i, i]) / (np.sum(cm) - np.sum(cm[:, i]))\n",
    "        for i in range(len(labels))\n",
    "    ])\n",
    "    \n",
    "    # Round metrics to three decimal places\n",
    "    accuracy = round(accuracy, 3)\n",
    "    precision = np.round(precision, 3)\n",
    "    recall = np.round(recall, 3)\n",
    "    f1 = np.round(f1, 3)\n",
    "    specificity = np.round(specificity, 3)\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    rows = []\n",
    "    for i, label in enumerate(labels):\n",
    "        rows.append({\n",
    "            'model': model_name,\n",
    "            'class': label,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1_score': f1[i],\n",
    "            'specificity': specificity[i]\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Create the ConfusionMatrices directory if it does not exist\n",
    "    folder_path = '../ImageLibrary/ConfusionMatrices'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Store the confusion matrix heatmap image\n",
    "    img_path = os.path.join(folder_path, f'confusion_matrix_{model_name}.png')\n",
    "    \n",
    "    # Define axis labels\n",
    "    axis_labels = ['Not Serious', 'Serious', 'Death']  # Replace with actual labels if different\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                     xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "                     annot_kws={\"size\": 11})\n",
    "    \n",
    "    # Set labels and title with larger font sizes\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.title(f'Classification Matrix Heatmap for {model_name}', fontsize=16)\n",
    "    \n",
    "    # Adjust the size of the tick labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)  # Adjust x-tick labels font size\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)  # Adjust y-tick labels font size\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to an image file\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "    \n",
    "    return metrics_df, img_path, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78116438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_coefficients_and_odds(data, model_name, top_n=5):\n",
    "    outcomes = ['Non-Serious', 'Serious', 'Death']\n",
    "    result = {}\n",
    "\n",
    "    for i, outcome in enumerate(outcomes):\n",
    "        # Intercept\n",
    "        intercept_data = {'Feature': ['Intercept'], 'LogOdds': [model_name.intercept_[i]]}\n",
    "        intercept_df = pd.DataFrame(intercept_data)\n",
    "        intercept_df['Odds'] = np.exp(intercept_df['LogOdds'])\n",
    "\n",
    "        # Coefficients\n",
    "        coef_data = {'Feature': data.columns, 'LogOdds': model_name.coef_[i]}\n",
    "        coef_df = pd.DataFrame(coef_data)\n",
    "        coef_df = coef_df.reindex(coef_df['LogOdds'].abs().sort_values(ascending=False).index)\n",
    "        coef_df['Odds'] = np.exp(coef_df['LogOdds'])\n",
    "\n",
    "        # Combine intercept and coefficients\n",
    "        df = pd.concat([intercept_df, coef_df])\n",
    "        \n",
    "        result[outcome] = df.head(top_n + 1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15dd837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ndc_list = [2405911,  # Dupixent\n",
    "                  502420150,  # Ocrevus\n",
    "                  595720402,  # Revlimid\n",
    "                  684620226]  # Ezetimibe\n",
    "drug_names = ['Dupixent', 'Ocrevus', 'Revlimid', 'Ezetimibe']\n",
    "nums = ['age', 'weight', 'unit_price']\n",
    "cats = ['sex', 'report_source', 'serious_outcome']\n",
    "\n",
    "def top_drug_comparison(input_ndc_list):\n",
    "    means = []\n",
    "    sds = []\n",
    "    counts = []\n",
    "\n",
    "    for drug in input_ndc_list:\n",
    "        drug_df = master_query_df.loc[master_query_df['ndc9'] == drug]\n",
    "        \n",
    "        # Calculate means and standard deviations for numeric columns\n",
    "        mean_values = drug_df[nums].mean()\n",
    "        sd_values = drug_df[nums].std()\n",
    "        \n",
    "        # Append to the list of means and standard deviations\n",
    "        means.append(mean_values)\n",
    "        sds.append(sd_values)\n",
    "        \n",
    "        # Calculate counts for categorical columns\n",
    "        count_values = {}\n",
    "        for col in cats:\n",
    "            value_counts = drug_df[col].value_counts(normalize=True).round(decimals=2)\n",
    "            # Sort by frequency (value) first, and by category (key) in case of ties\n",
    "            sorted_counts = dict(sorted(value_counts.items(), key=lambda item: (item[1], item[0])))\n",
    "            count_values[col] = sorted_counts\n",
    "        counts.append(pd.DataFrame.from_dict(count_values, orient='index').T)\n",
    "        \n",
    "    # Convert lists to DataFrames\n",
    "    means_df = pd.DataFrame(means, index=input_ndc_list)\n",
    "    sds_df = pd.DataFrame(sds, index=input_ndc_list)\n",
    "    num_desc = pd.concat([means_df, sds_df], axis=1, keys=['Mean', 'StdDev'])\n",
    "    num_desc['DrugNames'] = drug_names\n",
    "    \n",
    "    counts_df = pd.concat(counts, keys=input_ndc_list)\n",
    "    \n",
    "     # Add drug name labels to counts_df\n",
    "    counts_df['DrugName'] = counts_df.index.get_level_values(0).map(dict(zip(input_ndc_list, drug_names)))\n",
    "    \n",
    "    # Reset index to make drug name a column\n",
    "    counts_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Reorder columns so 'DrugName' comes first\n",
    "    cols = ['DrugName'] + [col for col in counts_df.columns if col != 'DrugName']\n",
    "    counts_df = counts_df[cols]\n",
    "    \n",
    "    return num_desc, counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c758197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_metrics_table(metrics_dfs):\n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(metrics_dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert DataFrame to a list of lists for tabulate\n",
    "    table = combined_df.values.tolist()\n",
    "    \n",
    "    # Get column headers from DataFrame\n",
    "    headers = combined_df.columns.tolist()\n",
    "    \n",
    "    # Generate and print the table\n",
    "    table_str = tabulate(table, headers, tablefmt='pretty')\n",
    "    print(table_str)\n",
    "    return combined_df\n",
    "\n",
    "#model_metrics = display_model_metrics_table([baseline_metrics_df,log_l1_metrics_df, log_l2_metrics_df, elastic_net_metrics_df,\n",
    "#                           tree1_metrics_df, rf_metrics_df, knn_metrics_df, grboost_metrics_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b2dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
